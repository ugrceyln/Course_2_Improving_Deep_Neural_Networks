{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Loading the Dataset](#2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $\\log(a^{[3]}) = \\log(0)$, the loss goes to infinity.\n",
    "- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. \n",
    "- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**In summary**:\n",
    "- Initializing weights to very large random values doesn't work well. \n",
    "- Initializing with small random values should do better. The important question is, how small should be these random values be? Let's find out up next!\n",
    "\n",
    "<font color='black'>    \n",
    "    \n",
    "**Optional Read:**\n",
    "\n",
    "\n",
    "The main difference between Gaussian variable (`numpy.random.randn()`) and uniform random variable is the distribution of the generated random numbers:\n",
    "\n",
    "- numpy.random.rand() produces numbers in a [uniform distribution](https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/rand.jpg).\n",
    "- and numpy.random.randn() produces numbers in a [normal distribution](https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/randn.jpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/1.JPG'>\n",
    "<img src='images/2.JPG'>\n",
    "<img src='images/3.JPG'>\n",
    "<img src='images/4.JPG'>\n",
    "<img src='images/5.JPG'>\n",
    "<img src='images/6.JPG'>\n",
    "<img src='images/7.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/8.JPG'>\n",
    "<img src='images/9.JPG'>\n",
    "<img src='images/10.JPG'>\n",
    "<img src='images/11.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/12.JPG'>\n",
    "<img src='images/13.JPG'>\n",
    "<img src='images/14.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/15.JPG'>\n",
    "<img src='images/16.JPG'>\n",
    "<img src='images/17.JPG'>\n",
    "<img src='images/18.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/19.JPG'>\n",
    "<img src='images/20.JPG'>\n",
    "<img src='images/21.JPG'>\n",
    "<img src='images/22.JPG'>\n",
    "<img src='images/23.JPG'>\n",
    "<img src='images/24.JPG'>\n",
    "<img src='images/25.JPG'>\n",
    "<img src='images/26.JPG'>\n",
    "<img src='images/27.JPG'>\n",
    "<img src='images/28.JPG'>\n",
    "<img src='images/29.JPG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (EnrichProjects)",
   "language": "python",
   "name": "pycharm-b64b4bc7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
